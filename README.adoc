= {subject}: {description}
Thomas Jungbauer <tjungbau@redhat.com>
:subject: OpenShift 4.x DR Tests
:description: Preparation Document
:projectname:
:customerlong: None
:customer: None
:consultantname: Thomas Jungbauer
:consultantmail: tjungbau@redhat.com
// asciidoctor knows also {firstname} {middlename} {lastname} {authorinitials}
:doctype: book
:confidentiality: Confidential; Restricted Distribution
:listing-caption: Listing
:toc:
:toclevels: 6
:numbered:
:chapter-label:
:encoding: UTF-8
:lang: en
:source-autofit:
//:pdf-fontsdir: /usr/share/fonts/liberation-sans;/usr/share/fonts/redhat;GEM_FONTS_DIR
:pdf-fontsdir: fonts/
:pdf-page-size: A4
:pdf-style: redhat
:pdf-stylesdir: pdf-styles/
:imagesdir: images/
ifdef::backend-pdf[]
:source-highlighter: rouge
:rouge-style: github
:icons: font
endif::[]
// only usable if you call `asciidoctor-pdf` with
//   --attribute gitdate=$(git log -1 --date=short --pretty=format:%cd)
// :revnumber: {gitdate}
:revnumber: 1.0.0
//A simple http://asciidoc.org[AsciiDoc] document.

## Disaster Recovery Test for single ETCD loss

### Prerequisites and assumptions

. You need a OpenShift Cluster. Duh!
. Be sure you have a etcd backup. Check this out: https://github.com/sushilsuresh/ocp4-ansible-roles/tree/master/roles/etcd-backup 
. Be sure you have the ignition files ready and you are able to re-create control plane machines.
. This manual is using local cluster based on *UPI* deployment and the command *virsh*. You might need to re-create the machines in a different way.


### Initial Setup

Initially 3 control planes and 3 worker nodes are running:

[source,bash]
----
virsh list --all

 Id    Name                           State
----------------------------------------------------
 2     ocp-master-0                   running
 3     ocp-master-1                   running
 4     ocp-master-2                   running
 5     ocp-compute-0                  running
 6     ocp-compute-1                  running
 7     ocp-compute-2                  running
----

### Verify an unhealthy etcd member

. Kill ocp-master-2
+
[source,bash]
----
virsh destroy ocp-master-2

# Output
Domain ocp-master-2 destroyed

# Check VMs
virsh list --all
setlocale: No such file or directory
 Id    Name                           State
----------------------------------------------------
 2     ocp-master-0                   running
 3     ocp-master-1                   running
 5     ocp-compute-0                  running
 6     ocp-compute-1                  running
 7     ocp-compute-2                  running
 -     ocp-master-2                   shut off
----
+
After a few moments the OpenShift cluster will realize that ocp-master-2 is lost and not available anymore. 

. Check status of etcd members
+
[source,bash]
----
oc get etcd -o=jsonpath='{range .items[0].status.conditions[?(@.type=="EtcdMembersAvailable")]}{.message}{"\n"}'

2 of 3 members are available, master-2 is unhealthy
----

. Check status of nodes - master-2 is NotReady
+
[source,bash]
----
oc get nodes
NAME        STATUS     ROLES           AGE   VERSION
compute-0   Ready      worker          19h   v1.18.3+2cf11e2
compute-1   Ready      worker          19h   v1.18.3+2cf11e2
compute-2   Ready      worker          19h   v1.18.3+2cf11e2
master-0    Ready      master,worker   20h   v1.18.3+2cf11e2
master-1    Ready      master,worker   20h   v1.18.3+2cf11e2
master-2    NotReady   master,worker   20h   v1.18.3+2cf11e2
----

. Check what is ahppening
.. Determine if the machine is not running
+
[source,bash]
----
oc get machines -A -ojsonpath='{range .items[*]}{@.status.nodeRef.name}{"\t"}{@.status.providerStatus.instanceState}{"\n"}' | grep -v running
----

.. Determine if the node is not ready
+
[source,bash]
----
oc get nodes -o jsonpath='{range .items[*]}{"\n"}{.metadata.name}{"\t"}{range .spec.taints[*]}{.key}{" "}' | grep unreachable

master-2	node.kubernetes.io/unreachable node.kubernetes.io/unreachable

# OR
oc get nodes
NAME        STATUS     ROLES           AGE   VERSION
compute-0   Ready      worker          19h   v1.18.3+2cf11e2
compute-1   Ready      worker          19h   v1.18.3+2cf11e2
compute-2   Ready      worker          19h   v1.18.3+2cf11e2
master-0    Ready      master,worker   20h   v1.18.3+2cf11e2
master-1    Ready      master,worker   20h   v1.18.3+2cf11e2
master-2    NotReady   master,worker   20h   v1.18.3+2cf11e2
----

### Replace an unhealthy etcd member

. Choose a Pod that is *NOT* on the affected node:
+
[source,bash]
----
oc get pods -n openshift-etcd | grep etcd

etcd-master-0                4/4     Running     0          19h
etcd-master-1                4/4     Running     0          19h
etcd-master-2                4/4     Running     0          19h
----
+
Since etcd-master-2 is not available, let's use _etcd-master-0_

. Login to etcd pod
+
[source,bash]
----
oc rsh -n openshift-etcd etcd-master-0

Defaulting container name to etcdctl.
Use 'oc describe pod/etcd-master-0 -n openshift-etcd' to see all of the containers in this pod.
----

. View etcd member list
+
[source,bash]
----
sh-4.2# etcdctl member list -w table
----
+
The output should be something like this:
+
[cols="6",options=header]
|===
|ID
|STATUS
|NAME
|PEER ADDRS 
|CLIENT ADDRS
|IS LEARNER

|242683dbf854c077
|started
|master-2
|https://192.168.50.12:2380
|https://192.168.50.12:2379 
|false

|325d137f90a90ffa
|started
|master-0
|https://192.168.50.10:2380
|https://192.168.50.10:2379
|false

|3fa2aa4f96d18eac
|started
|master-1
|https://192.168.50.11:2380 
|https://192.168.50.11:2379
|false

|===

. Remove broken member - 242683dbf854c077
+
[source,bash]
----
sh-4.2# etcdctl member remove 242683dbf854c077
Member 242683dbf854c077 removed from cluster 7ea00afa4db9962c

sh-4.2# etcdctl member list -w table
----
+
[cols="6",options=header]
|===
|ID
|STATUS
|NAME
|PEER ADDRS 
|CLIENT ADDRS
|IS LEARNER

|325d137f90a90ffa
|started
|master-0
|https://192.168.50.10:2380
|https://192.168.50.10:2379
|false

|3fa2aa4f96d18eac
|started
|master-1
|https://192.168.50.11:2380 
|https://192.168.50.11:2379
|false

|===

. Delete broken node from cluster
+
[source,bash]
----
oc delete node master-2
node "master-2" deleted
----

. Remove VM
+
[source,bash]
----

virsh list --all
 Id    Name                           State
----------------------------------------------------
 2     ocp-master-0                   running
 3     ocp-master-1                   running
 5     ocp-compute-0                  running
 6     ocp-compute-1                  running
 7     ocp-compute-2                  running
 -     ocp-master-2                   shut off

virsh undefine ocp-master-2
Domain ocp-master-2 has been undefined

virsh list --all
 Id    Name                           State
----------------------------------------------------
 2     ocp-master-0                   running
 3     ocp-master-1                   running
 5     ocp-compute-0                  running
 6     ocp-compute-1                  running
 7     ocp-compute-2                  running
----

. Create new Control Plane node
This procedure depends on your specific environment. Be sure that that the ignition files are ready and that you approve the CSRs, which are created few minutes after the new node has been registered at the cluster.
+
[source,bash]
----
virsh list --all
 Id    Name                           State
----------------------------------------------------
 2     ocp-master-0                   running
 3     ocp-master-1                   running
 5     ocp-compute-0                  running
 6     ocp-compute-1                  running
 7     ocp-compute-2                  running
 8     ocp-master-2                   running
----

.. Approve certificates
+
[source,bash]
----
oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs oc adm certificate approve
----

. Check if new node is ready again
+
[source,bash]
----
oc get nodes
NAME        STATUS   ROLES           AGE   VERSION
compute-0   Ready    worker          20h   v1.18.3+2cf11e2
compute-1   Ready    worker          20h   v1.18.3+2cf11e2
compute-2   Ready    worker          20h   v1.18.3+2cf11e2
master-0    Ready    master,worker   20h   v1.18.3+2cf11e2
master-1    Ready    master,worker   20h   v1.18.3+2cf11e2
master-2    Ready    master,worker   88s   v1.18.3+2cf11e2
----

. Wait until all cluster operators are up and running again.
+
This might take several minutes.
+
[source,bash]
----
oc get clusteroperator
NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE
authentication                             4.5.7     True        False         False      19h
cloud-credential                           4.5.7     True        False         False      20h
cluster-autoscaler                         4.5.7     True        False         False      20h
config-operator                            4.5.7     True        False         False      20h
console                                    4.5.7     True        False         False      18h
csi-snapshot-controller                    4.5.7     True        False         False      18h
dns                                        4.5.7     True        False         False      20h
etcd                                       4.5.7     True        True          False      20h
image-registry                             4.5.7     True        False         False      19h
ingress                                    4.5.7     True        False         False      20h
insights                                   4.5.7     True        False         False      20h
kube-apiserver                             4.5.7     True        True          False      20h
kube-controller-manager                    4.5.7     True        True          False      20h
kube-scheduler                             4.5.7     True        False         False      20h
kube-storage-version-migrator              4.5.7     True        False         False      2m57s
machine-api                                4.5.7     True        False         False      20h
machine-approver                           4.5.7     True        False         False      20h
machine-config                             4.5.7     True        False         False      12s
marketplace                                4.5.7     True        False         False      18h
monitoring                                 4.5.7     False       True          True       16m
network                                    4.5.7     True        False         False      20h
node-tuning                                4.5.7     True        False         False      19h
openshift-apiserver                        4.5.7     False       True          True       2m35s
openshift-controller-manager               4.5.7     True        False         False      69m
openshift-samples                          4.5.7     True        False         False      19h
operator-lifecycle-manager                 4.5.7     True        False         False      20h
operator-lifecycle-manager-catalog         4.5.7     True        False         False      20h
operator-lifecycle-manager-packageserver   4.5.7     False       True          False      87s
service-ca                                 4.5.7     True        False         False      20h
storage                                    4.5.7     True        False         False      19h
----

### Verify etcd

. Login into one etcd member and check the available members
+
[source,bash]
----
oc rsh -n openshift-etcd etcd-master-0

sh-4.2# etcdctl member list -w table
----
+
[cols="6",options=header]
|===
|ID
|STATUS
|NAME
|PEER ADDRS 
|CLIENT ADDRS
|IS LEARNER

|df30cf8269b9463a
|started
|master-2
|https://192.168.50.12:2380
|https://192.168.50.12:2379 
|false

|325d137f90a90ffa
|started
|master-0
|https://192.168.50.10:2380
|https://192.168.50.10:2379
|false

|3fa2aa4f96d18eac
|started
|master-1
|https://192.168.50.11:2380 
|https://192.168.50.11:2379
|false

|===

. Check etcd cluster health
+
[source,bash]
----
oc rsh -n openshift-etcd etcd-master-0
Defaulting container name to etcdctl.
Use 'oc describe pod/etcd-master-0 -n openshift-etcd' to see all of the containers in this pod.

sh-4.2# etcdctl endpoint health --cluster
https://192.168.50.10:2379 is healthy: successfully committed proposal: took = 24.450739ms
https://192.168.50.12:2379 is healthy: successfully committed proposal: took = 35.904635ms
https://192.168.50.11:2379 is healthy: successfully committed proposal: took = 36.332911ms
----